{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from ethos.constants import PROJECT_ROOT\n",
    "from ethos.constants import SpecialToken as ST\n",
    "\n",
    "data_dir = PROJECT_ROOT / \"data/tokenized_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.datasets import HospitalMortalityDataset\n",
    "\n",
    "d_hosp = HospitalMortalityDataset(data_dir / \"mimic_old_ed/test\")\n",
    "d_hosp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.datasets import ICUAdmissionDataset\n",
    "\n",
    "d_icu = ICUAdmissionDataset(data_dir / \"mimic_old_ed/test\")\n",
    "d_icu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Find patients for generating trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static_data(dataset, idx):\n",
    "    patient_id = dataset.patient_id_at_idx[idx].item()\n",
    "    time_at_start = dataset.times[idx].item()\n",
    "    static_data = dataset.static_data[patient_id]\n",
    "    dt = timedelta(microseconds=time_at_start - static_data[\"MEDS_BIRTH\"][\"time\"][0])\n",
    "    age_years = min(dt.days / 365.25, 99)\n",
    "    marital = static_data[\"MARITAL\"][\"code\"][-1]\n",
    "    gender = static_data[\"GENDER\"][\"code\"][-1]\n",
    "    race = static_data[\"RACE\"][\"code\"][-1]\n",
    "    return {\n",
    "        \"age\": age_years,\n",
    "        \"marital\": marital,\n",
    "        \"gender\": gender,\n",
    "        \"race\": race,\n",
    "    }\n",
    "\n",
    "\n",
    "def dataset_to_df(dataset, n):\n",
    "    return (\n",
    "        pl.DataFrame((dataset[i][1] for i in range(n)), infer_schema_length=1_000_000, orient=\"row\")\n",
    "        .with_row_index()\n",
    "        .with_columns(pl.col(\"^.*time.*$\").cast(pl.Duration))\n",
    "    )\n",
    "\n",
    "\n",
    "n = len(d_hosp)\n",
    "df_hosp = dataset_to_df(d_hosp, n)\n",
    "df_icu = dataset_to_df(d_icu, n)\n",
    "df_hosp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"index\", \"expected\", \"true_token_dist\", \"true_token_time\", \"patient_id\", \"data_idx\"]\n",
    "\n",
    "token_dist_expr = pl.col(\"true_token_dist\").is_between(500, 1500)\n",
    "df_hosp = df_hosp.filter(token_dist_expr)[columns]\n",
    "df_icu = df_icu.filter(token_dist_expr)[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_sample = 10\n",
    "trajectories_patients = pl.concat(\n",
    "    [\n",
    "        df_hosp.filter(expected=ST.DISCHARGE).sample(n_sample, seed=seed),\n",
    "        df_hosp.filter(expected=ST.DEATH).sample(n_sample, seed=seed),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trajectories_patients = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            trajectories_patients,\n",
    "            df_icu.filter(\n",
    "                ~pl.col(\"index\").is_in(trajectories_patients[\"index\"]), expected=ST.ICU_ADMISSION\n",
    "            ).sample(n_sample, seed=seed),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        static_data=pl.col(\"data_idx\").map_elements(\n",
    "            lambda idx: get_static_data(d_hosp, idx), return_dtype=pl.Struct\n",
    "        ),\n",
    "    )\n",
    "    .unnest(\"static_data\")\n",
    "    .sort(\"index\")\n",
    ")\n",
    "\n",
    "trajectories_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.constants import PROJECT_ROOT\n",
    "\n",
    "results_dir = PROJECT_ROOT / \"results\" / \"trajectories\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "trajectories_patients.select(pl.exclude(\"true_token_time\")).write_csv(\n",
    "    results_dir / \"trajectories_patients.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Generate trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from ethos.constants import PROJECT_ROOT\n",
    "from ethos.constants import SpecialToken as ST\n",
    "from ethos.inference.constants import Reason, Task\n",
    "from ethos.metrics import preprocess_inference_results\n",
    "\n",
    "results_hosp = PROJECT_ROOT / \"results\" / Task.HOSPITAL_MORTALITY_SINGLE\n",
    "results_icu = PROJECT_ROOT / \"results\" / Task.ICU_ADMISSION_SINGLE\n",
    "results_trajectories = PROJECT_ROOT / \"results\" / \"trajectories\"\n",
    "patients_info = pl.read_csv(results_trajectories / \"trajectories_patients.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "prolonged_stay_cutoff = timedelta(days=10)\n",
    "prolonged_stay_cutoff2 = timedelta(days=15)\n",
    "\n",
    "first_cutoff_condition = (\n",
    "    pl.col(\"true_token_time\").first() - pl.col(\"true_token_time\") < prolonged_stay_cutoff\n",
    ")\n",
    "first_cutoff = pl.col(\"token_time\") >= prolonged_stay_cutoff - (\n",
    "    pl.col(\"true_token_time\").first() - pl.col(\"true_token_time\")\n",
    ")\n",
    "second_cutoff = pl.col(\"token_time\") >= prolonged_stay_cutoff2 - (\n",
    "    pl.col(\"true_token_time\").first() - pl.col(\"true_token_time\")\n",
    ")\n",
    "\n",
    "config_by_task = {\n",
    "    ST.DEATH: {\n",
    "        \"actual_expr\": pl.col(\"actual\").is_in([ST.DEATH]),\n",
    "        \"expected_expr\": pl.col(\"expected\").is_in([ST.DEATH]),\n",
    "    },\n",
    "    ST.ICU_ADMISSION: {\n",
    "        \"actual_expr\": pl.col(\"actual\").is_in([ST.ICU_ADMISSION]),\n",
    "        \"expected_expr\": pl.col(\"expected\").is_in([ST.ICU_ADMISSION]),\n",
    "    },\n",
    "    \"COMPOSITE\": {\n",
    "        \"actual_expr\": pl.col(\"actual\").is_in([ST.ICU_ADMISSION, ST.DEATH])\n",
    "        | (pl.when(first_cutoff_condition).then(first_cutoff).otherwise(second_cutoff)),\n",
    "        \"expected_expr\": pl.col(\"expected\").is_in([ST.ICU_ADMISSION, ST.DEATH])\n",
    "        | (pl.when(first_cutoff_condition).then(first_cutoff).otherwise(second_cutoff)),\n",
    "    },\n",
    "    \"PROLONGED_STAY\": {\n",
    "        \"actual_expr\": (\n",
    "            pl.when(first_cutoff_condition).then(first_cutoff).otherwise(second_cutoff)\n",
    "        ),\n",
    "        \"expected_expr\": (\n",
    "            pl.when(first_cutoff_condition).then(first_cutoff).otherwise(second_cutoff)\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class TrajectoryType(StrEnum):\n",
    "    TOKEN = \"TOKEN\"\n",
    "    TIME = \"TIME\"\n",
    "    TOKEN_GROUPED = \"TOKEN_GROUPED\"\n",
    "\n",
    "\n",
    "def group_tokens_by_piece_of_info(df: pl.DataFrame):\n",
    "    full_tokens = {\n",
    "        \"ED_REGISTRATION\": 2,\n",
    "        \"ED_ACUITY\": 2,\n",
    "        \"HOSPITAL_ADMISSION\": 2,\n",
    "        \"ICU_ADMISSION\": 2,\n",
    "        \"SOFA\": 2,\n",
    "    }\n",
    "\n",
    "    prefix_tokens = {\n",
    "        \"ICD//PCS\": 7,\n",
    "        \"ICD//CM\": 3,\n",
    "        \"VITAL\": 2,\n",
    "        \"LAB\": 2,\n",
    "        \"ATC\": 3,\n",
    "    }\n",
    "\n",
    "    tokens = df[\"start_token\"].to_list()\n",
    "    counter = iter(range(len(tokens)))\n",
    "    groups = []\n",
    "    while tokens:\n",
    "        number_of_pops = 1\n",
    "        token, group_numer = tokens[0], next(counter)\n",
    "        if token in full_tokens:\n",
    "            number_of_pops = full_tokens[token]\n",
    "        elif prefix := [prefix for prefix in prefix_tokens if token.startswith(prefix)]:\n",
    "            number_of_pops = prefix_tokens[prefix[0]]\n",
    "\n",
    "        for _ in range(number_of_pops):\n",
    "            groups.append(group_numer)\n",
    "            tokens.pop(0)\n",
    "\n",
    "    return df.group_by(by=pl.Series(groups).set_sorted()).agg(\n",
    "        pl.col(\"start_token\").str.join(\" \"),\n",
    "        pl.exclude(\"start_token\", \"true_token_time\").last(),\n",
    "        pl.first(\"true_token_time\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def format_duration(duration):\n",
    "    days = duration // 1_000_000 // 86400\n",
    "    hours = (duration // 1_000_000 % 86400) // 3600\n",
    "\n",
    "    parts = []\n",
    "    if days > 0:\n",
    "        parts.append(f\"{days}d\")\n",
    "    if hours > 0:\n",
    "        parts.append(f\"{hours}h\")\n",
    "    if len(parts) == 0:\n",
    "        return \"0h\"\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "time_label_expr_one_liner_d = (\n",
    "    \"(floor(datum.value / 1e6 / 86400) > 0 ? floor(datum.value / 1e6 / 86400) + 'd ' : '')\"\n",
    ")\n",
    "time_label_expr_one_liner_d0 = \"floor(datum.value / 1e6 / 86400) + 'd')\"\n",
    "\n",
    "time_label_expr_one_liner_d_h = (\n",
    "    time_label_expr_one_liner_d\n",
    "    + \" +  (floor(datum.value / 1e6 % 86400 / 3600) > 0 ? floor(datum.value / 1e6 % 86400 / 3600) + 'h ' : '')\"\n",
    ")\n",
    "\n",
    "time_label_expr_one_liner_d_h_m = (\n",
    "    time_label_expr_one_liner_d_h\n",
    "    + \" + (floor(datum.value / 1e6 % 3600 / 60) > 0 ? floor(datum.value / 1e6 % 3600 / 60) + 'min ' : '')\"\n",
    ")\n",
    "color_gradient = alt.Color(\n",
    "    \"actual\", legend=None, scale=alt.Scale(domain=[0, 0.5, 1], range=[\"lightgrey\", \"orange\", \"red\"])\n",
    ")\n",
    "\n",
    "\n",
    "def process_df_for_display(df):\n",
    "    return (\n",
    "        df.with_row_index(\"index\")\n",
    "        .with_columns(\n",
    "            actual_diff=pl.col(\"actual\").diff(),\n",
    "            actual_display=pl.when(pl.col(\"actual\") == 1.0)\n",
    "            .then(pl.lit(\"1.0\"))\n",
    "            .otherwise(\n",
    "                pl.col(\"actual\")\n",
    "                .round(2)\n",
    "                .cast(pl.String)\n",
    "                .str.slice(1)\n",
    "                .str.replace(r\"(\\.\\d)$\", r\"${1}\" + \"0\")\n",
    "            ),\n",
    "            start_token_display=pl.col(\"start_token\").str.replace(\"//\", \"-\").str.replace(\"//\", \"-\"),\n",
    "            zero=0,\n",
    "            true_token_time_duration=(\n",
    "                pl.col(\"true_token_time\").first() - pl.col(\"true_token_time\")\n",
    "            ).cast(pl.UInt64),\n",
    "            true_token_time=pl.col(\"true_token_time\").cast(pl.UInt64),\n",
    "            token_time=pl.col(\"token_time\").cast(pl.UInt64),\n",
    "            prob_error=((pl.col(\"actual\") * (1 - pl.col(\"actual\"))) / pl.col(\"counts\")).sqrt(),\n",
    "        )\n",
    "        .with_columns(\n",
    "            start_token_display=pl.when(pl.col(\"start_token_display\").str.len_chars() > 15)\n",
    "            .then(pl.col(\"start_token_display\").str.slice(0, 15) + \"...\")\n",
    "            .otherwise(pl.col(\"start_token_display\")),\n",
    "            true_token_time_duration_display=pl.col(\"true_token_time_duration\").map_elements(\n",
    "                format_duration, return_dtype=pl.String\n",
    "            ),\n",
    "            actual_plus_error=pl.min_horizontal(\n",
    "                pl.col(\"actual\") + pl.col(\"prob_error\") * 1.96, pl.lit(1)\n",
    "            ),\n",
    "            actual_minus_error=pl.col(\"actual\") - pl.col(\"prob_error\") * 1.96,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def inference_results_for_token_of_interest(\n",
    "    dir_name, token_of_interest=ST.ICU_ADMISSION, trajectory_type=TrajectoryType.TOKEN\n",
    "):\n",
    "    df = preprocess_inference_results(\n",
    "        dir_name,\n",
    "        actual_expr=config_by_task[token_of_interest][\"actual_expr\"],\n",
    "        expected_expr=config_by_task[token_of_interest][\"expected_expr\"],\n",
    "        filter_ambiguous=(\n",
    "            ~pl.col(\"actual\").is_in([ST.TIMELINE_END])\n",
    "            & pl.col(\"stop_reason\").is_in([Reason.GOT_TOKEN])\n",
    "        ),\n",
    "        additional_columns=[\"start_token\"],\n",
    "        warn_on_dropped=False,\n",
    "    ).sort(\"data_idx\")\n",
    "\n",
    "    match trajectory_type:\n",
    "        case TrajectoryType.TOKEN:\n",
    "            return process_df_for_display(df)\n",
    "        case TrajectoryType.TIME:\n",
    "            df = df.group_by(\"true_token_time\").agg(pl.all().last()).sort(\"data_idx\")\n",
    "        case TrajectoryType.TOKEN_GROUPED:\n",
    "            df = group_tokens_by_piece_of_info(df).sort(\"data_idx\")\n",
    "\n",
    "    return process_df_for_display(df)\n",
    "\n",
    "\n",
    "def create_token_view(df, brushes, width, trajectory_type):\n",
    "    spacing = 10\n",
    "    width_token_view = (width - (spacing * len(brushes) - 1)) / len(brushes)\n",
    "    match trajectory_type:\n",
    "        case TrajectoryType.TOKEN:\n",
    "            x_col = \"index\"\n",
    "        case TrajectoryType.TIME:\n",
    "            x_col = \"true_token_time_duration\"\n",
    "        case TrajectoryType.TOKEN_GROUPED:\n",
    "            x_col = \"index\"\n",
    "        case _:\n",
    "            raise ValueError(trajectory_type)\n",
    "    result = None\n",
    "    for brush in brushes:\n",
    "        base_token_view = alt.Chart(df).encode(\n",
    "            x=alt.X(\n",
    "                x_col,\n",
    "                axis=alt.Axis(grid=False, title=None, format=\"d\", labels=False, ticks=False),\n",
    "                scale=alt.Scale(domain=brush),\n",
    "            ),\n",
    "            y=alt.Y(\"zero:Q\", scale=alt.Scale(domain=[-0.2, 0.2])),\n",
    "        )\n",
    "\n",
    "        line = base_token_view.mark_line(size=1, color=\"#5a5255\")\n",
    "        squares = base_token_view.mark_square(size=400, opacity=1).encode(color=color_gradient)\n",
    "\n",
    "        actual_text = base_token_view.mark_text(dy=5, fontSize=10, color=\"black\").encode(\n",
    "            text=alt.Text(\"actual_display\")\n",
    "        )\n",
    "\n",
    "        start_token_text = (\n",
    "            base_token_view.mark_text(fontSize=10)\n",
    "            .transform_calculate(y_adjusted=\"datum.zero + (datum.index % 2 == 0 ? -0.15 : 0.15)\")\n",
    "            .encode(\n",
    "                y=alt.Y(\n",
    "                    \"y_adjusted:Q\",\n",
    "                    axis=alt.Axis(grid=False, title=None, labels=False, ticks=False),\n",
    "                    scale=alt.Scale(domain=[-0.2, 0.2]),\n",
    "                ),\n",
    "                text=\"start_token_display:N\",\n",
    "                color=alt.value(\"black\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        token_view = (line + squares + actual_text + start_token_text).properties(\n",
    "            width=width_token_view, height=60\n",
    "        )\n",
    "        result = token_view if result is None else alt.hconcat(result, token_view, spacing=spacing)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_main_chart(\n",
    "    dfs,\n",
    "    labels,\n",
    "    width,\n",
    "    trajectory_type,\n",
    "    return_focused=False,\n",
    "    token_view_window_num=1,\n",
    "    token_view_ranges=None,\n",
    "):\n",
    "    full = None\n",
    "    focused = None\n",
    "\n",
    "    match trajectory_type:\n",
    "        case TrajectoryType.TOKEN:\n",
    "            x_col = \"index\"\n",
    "            x_title = \"Token Number\"\n",
    "            x_format = \"d\"\n",
    "            labelExpr_full = labelExpr_focused = \"datum.value\"\n",
    "        case TrajectoryType.TIME:\n",
    "            x_col = \"true_token_time_duration\"\n",
    "            x_title = \"Time\"\n",
    "            x_format = \"\"\n",
    "            labelExpr_full = time_label_expr_one_liner_d_h\n",
    "            labelExpr_focused = time_label_expr_one_liner_d_h_m\n",
    "        case TrajectoryType.TOKEN_GROUPED:\n",
    "            x_col = \"index\"\n",
    "            x_title = \"Token Grouped\"\n",
    "            x_format = \"d\"\n",
    "            labelExpr_full = labelExpr_focused = \"datum.value\"\n",
    "        case _:\n",
    "            raise ValueError(trajectory_type)\n",
    "\n",
    "    if token_view_ranges is not None and len(token_view_ranges) != token_view_window_num:\n",
    "        raise ValueError(\n",
    "            f\"Length of ranges expected {token_view_window_num}, got {len(token_view_ranges)}\"\n",
    "        )\n",
    "\n",
    "    if token_view_ranges is None:\n",
    "        token_view_ranges = []\n",
    "        step = len(dfs[-1]) // (token_view_window_num + 1)\n",
    "        for i in range(token_view_window_num):\n",
    "            r = step * (i + 1) - 2.5, step * (i + 1) + 2.5\n",
    "            token_view_ranges.append(r)\n",
    "\n",
    "    x_min, x_max = dfs[0][\"index\"].min(), dfs[0][\"index\"].max()\n",
    "\n",
    "    for idx, (df, label) in enumerate(zip(dfs, labels)):\n",
    "        color_scale = alt.Scale(\n",
    "            domain=[\"COMPOSITE\", \"DEATH\", \"ICU ADMISSION\", \"PROLONGED STAY\"],\n",
    "            range=[\"purple\", \"#bf5b17\", \"#386cb0\", \"#666666\"],\n",
    "        )\n",
    "        brushes = [\n",
    "            alt.selection_interval(encodings=[\"x\"], value={\"x\": r}) for r in token_view_ranges\n",
    "        ]\n",
    "        df = df.with_columns(Risk=pl.lit(label))\n",
    "\n",
    "        error_band = (\n",
    "            alt.Chart(df)\n",
    "            .mark_area(opacity=0.3)\n",
    "            .encode(\n",
    "                x=alt.X(\n",
    "                    x_col,\n",
    "                    title=x_title,\n",
    "                    axis=alt.Axis(grid=False),\n",
    "                    scale=alt.Scale(domain=[x_min, x_max]),\n",
    "                ),\n",
    "                y=alt.Y(\n",
    "                    \"actual_minus_error\",\n",
    "                    title=None,\n",
    "                    axis=alt.Axis(grid=False),\n",
    "                    scale=alt.Scale(domain=[0.0, 1.0]),\n",
    "                ),\n",
    "                y2=alt.Y2(\"actual_plus_error\"),\n",
    "                color=alt.Color(\"Risk:N\", scale=color_scale),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        full_view = (\n",
    "            alt.Chart(df, width=width, height=150)\n",
    "            .mark_line(strokeWidth=1.3)\n",
    "            .encode(\n",
    "                x=alt.X(\n",
    "                    x_col,\n",
    "                    title=x_title,\n",
    "                    axis=alt.Axis(grid=False, labelExpr=labelExpr_full),\n",
    "                    scale=alt.Scale(domain=[x_min, x_max]),\n",
    "                ),\n",
    "                y=alt.Y(\n",
    "                    \"actual\",\n",
    "                    title=None,\n",
    "                    axis=alt.Axis(grid=False),\n",
    "                    scale=alt.Scale(domain=[0.0, 1.0]),\n",
    "                ),\n",
    "                color=alt.Color(\n",
    "                    \"Risk:N\",\n",
    "                    scale=color_scale,\n",
    "                    legend=alt.Legend(orient=\"none\", legendX=5, legendY=5, title=None, padding=5),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        full_view = (error_band + full_view).add_params(*brushes)\n",
    "\n",
    "        if return_focused:\n",
    "            focused_view = (\n",
    "                alt.Chart(df, width=width, height=200)\n",
    "                .mark_line()\n",
    "                .encode(\n",
    "                    x=alt.X(\n",
    "                        x_col,\n",
    "                        title=None,\n",
    "                        axis=alt.Axis(format=x_format, grid=False, labelExpr=labelExpr_focused),\n",
    "                        scale=alt.Scale(domain=brushes[0]),\n",
    "                    ),\n",
    "                    y=alt.Y(\n",
    "                        \"actual\",\n",
    "                        title=None,\n",
    "                        axis=alt.Axis(grid=False),\n",
    "                        scale=alt.Scale(domain=[0.0, 1.0]),\n",
    "                    ),\n",
    "                    color=alt.Color(\"Risk:N\", title=\"Risk Score\", legend=None),\n",
    "                )\n",
    "            )\n",
    "            focused = focused_view if focused is None else focused + focused_view\n",
    "        full = full_view if full is None else full + full_view\n",
    "\n",
    "    if return_focused:\n",
    "        return focused & full, brushes\n",
    "\n",
    "    return full, brushes\n",
    "\n",
    "\n",
    "def sample_df_and_transform_for_rect_drawing(\n",
    "    df, indices, offset, m1_offset, p1_offset, block_width, shift\n",
    "):\n",
    "    return (\n",
    "        df.filter(pl.col(\"index\").is_in(indices))\n",
    "        .with_row_index(\"n_index\")\n",
    "        .with_columns(\n",
    "            n_index=pl.col(\"index\") + offset * block_width - shift,\n",
    "            n_index_p1=(pl.col(\"index\") + offset * block_width) + block_width - shift,\n",
    "            m1=(pl.col(\"zero\") + m1_offset),\n",
    "            p1=(pl.col(\"zero\") + p1_offset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_ares_block(df, x_col, x2_col, y_col, y2_col, n_samples, scale):\n",
    "    return (\n",
    "        alt.Chart(df)\n",
    "        .mark_rect(opacity=1, cornerRadius=10)\n",
    "        .encode(\n",
    "            x=alt.X(\n",
    "                f\"{x_col}:Q\",\n",
    "                scale=alt.Scale(domain=[0, scale]),\n",
    "                axis=alt.Axis(\n",
    "                    grid=False, title=None, orient=\"top\", labels=False, ticks=False, domain=False\n",
    "                ),\n",
    "            ),\n",
    "            x2=f\"{x2_col}:Q\",\n",
    "            y=alt.Y(\n",
    "                f\"{y_col}:Q\",\n",
    "                scale=alt.Scale(domain=[-2.0, 1.5]),\n",
    "                axis=alt.Axis(grid=False, title=None, labels=False, ticks=False, domain=False),\n",
    "            ),\n",
    "            y2=f\"{y2_col}:Q\",\n",
    "            color=color_gradient,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_timeline(df, tokens_num):\n",
    "    arrow = alt.layer(\n",
    "        alt.Chart()\n",
    "        .mark_line(size=2)\n",
    "        .encode(\n",
    "            x=alt.datum(0, scale=alt.Scale(domain=[0, tokens_num])),\n",
    "            y=alt.datum(-1.25),\n",
    "            x2=alt.datum(tokens_num),\n",
    "            y2=alt.datum(-1.30),\n",
    "        ),\n",
    "        alt.Chart()\n",
    "        .mark_point(shape=\"triangle\", filled=True, fillOpacity=1)\n",
    "        .encode(\n",
    "            x=alt.datum(tokens_num, scale=alt.Scale(domain=[0, tokens_num])),\n",
    "            y=alt.datum(-1.275),\n",
    "            angle=alt.AngleValue(90),\n",
    "            size=alt.SizeValue(100),\n",
    "            color=alt.ColorValue(\"#000000\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    dots = (\n",
    "        alt.Chart(df)\n",
    "        .mark_point(filled=True, fillOpacity=1)\n",
    "        .encode(\n",
    "            x=alt.X(\n",
    "                \"index:Q\",\n",
    "                axis=alt.Axis(grid=False, title=None, labels=False, ticks=False),\n",
    "                scale=alt.Scale(domain=[0, tokens_num]),\n",
    "            ),\n",
    "            y=alt.datum(-1.275),\n",
    "            size=alt.SizeValue(80),\n",
    "            color=alt.ColorValue(\"#000000\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    texts = (\n",
    "        alt.Chart(df)\n",
    "        .mark_text(fillOpacity=1)\n",
    "        .encode(\n",
    "            x=alt.X(\n",
    "                \"index:Q\",\n",
    "                axis=alt.Axis(grid=False, title=None, labels=False, ticks=False),\n",
    "                scale=alt.Scale(domain=[0, tokens_num]),\n",
    "            ),\n",
    "            text=\"true_token_time_duration_display\",\n",
    "            y=alt.datum(-1.825),\n",
    "            size=alt.SizeValue(11),\n",
    "            color=alt.ColorValue(\"#000000\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return arrow + dots + texts\n",
    "\n",
    "\n",
    "def create_ares_overview(dfs, width):\n",
    "    n_samples = 10\n",
    "    block_width = len(dfs[-1]) / (n_samples * 3 - 1)\n",
    "    shift = block_width // 2\n",
    "    indices = (\n",
    "        np.linspace(0, len(dfs[-1]) - 1 + (len(dfs[-1]) / n_samples / 3), n_samples + 1, dtype=int)\n",
    "        + shift\n",
    "    )[:-1]\n",
    "\n",
    "    tokens_num = len(dfs[-1])\n",
    "\n",
    "    offsets = [(1, 0.5, 1), (1, 0.25, -0.25), (1, -1, -0.5), (0, -1, 1)]\n",
    "    sampled_dfs = [\n",
    "        sample_df_and_transform_for_rect_drawing(df, indices, *offset, block_width, shift)\n",
    "        for df, offset in zip(dfs, offsets)\n",
    "    ]\n",
    "    charts = [\n",
    "        create_ares_block(df, \"n_index\", \"n_index_p1\", \"m1\", \"p1\", n_samples, tokens_num)\n",
    "        for df in sampled_dfs\n",
    "    ]\n",
    "\n",
    "    timeline = create_timeline(sampled_dfs[-1], tokens_num)\n",
    "\n",
    "    return alt.layer(timeline, *charts).properties(\n",
    "        width=width, height=70, title=\"ARES\", view=alt.ViewConfig(stroke=None)\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_adaptive_ews(\n",
    "    dir_name_hosp,\n",
    "    dir_name_icu,\n",
    "    trajectory_type=TrajectoryType.TOKEN,\n",
    "    token_view_window_num=1,\n",
    "    token_view_ranges=None,\n",
    "    return_dfs=False,\n",
    "):\n",
    "    width = 800\n",
    "    df_death = inference_results_for_token_of_interest(dir_name_hosp, ST.DEATH, trajectory_type)\n",
    "    df_icu = inference_results_for_token_of_interest(\n",
    "        dir_name_icu, ST.ICU_ADMISSION, trajectory_type\n",
    "    )\n",
    "    df_prolonged = inference_results_for_token_of_interest(\n",
    "        dir_name_hosp, \"PROLONGED_STAY\", trajectory_type\n",
    "    )\n",
    "    # TODO using hospital trajectory for compound as it is longer\n",
    "    df_compound = inference_results_for_token_of_interest(\n",
    "        dir_name_hosp, \"COMPOSITE\", trajectory_type\n",
    "    )\n",
    "\n",
    "    icu_values = df_icu[\"actual\"].to_list() + [0.0] * (len(df_death) - len(df_icu))\n",
    "    df_icu_extended = pl.Series(\"actual_icu\", icu_values, dtype=pl.Float64)\n",
    "\n",
    "    df_compound = (\n",
    "        df_compound.with_columns(\n",
    "            actual=pl.min_horizontal(\n",
    "                [df_death[\"actual\"] + df_prolonged[\"actual\"] + df_icu_extended, pl.lit(1)]\n",
    "            )\n",
    "        )\n",
    "        .with_columns(\n",
    "            prob_error=((pl.col(\"actual\") * (1 - pl.col(\"actual\"))) / pl.col(\"counts\")).sqrt(),\n",
    "            actual_display=pl.when(pl.col(\"actual\") == 1.0)\n",
    "            .then(pl.lit(\"1.0\"))\n",
    "            .otherwise(\n",
    "                pl.col(\"actual\")\n",
    "                .round(2)\n",
    "                .cast(pl.String)\n",
    "                .str.slice(1)\n",
    "                .str.replace(r\"(\\.\\d)$\", r\"${1}\" + \"0\")\n",
    "            ),\n",
    "        )\n",
    "        .with_columns(\n",
    "            actual_plus_error=pl.min_horizontal(\n",
    "                pl.col(\"actual\") + pl.col(\"prob_error\") * 1.96, pl.lit(1)\n",
    "            ),\n",
    "            actual_minus_error=pl.col(\"actual\") - pl.col(\"prob_error\") * 1.96,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dfs = [df_death, df_icu, df_prolonged, df_compound]\n",
    "    labels = [\"DEATH\", \"ICU ADMISSION\", \"PROLONGED STAY\", \"COMPOSITE\"]\n",
    "\n",
    "    main_chart, brushes = create_main_chart(\n",
    "        dfs,\n",
    "        labels,\n",
    "        width,\n",
    "        trajectory_type,\n",
    "        token_view_window_num=token_view_window_num,\n",
    "        token_view_ranges=token_view_ranges,\n",
    "    )\n",
    "    token_view = create_token_view(df_compound, brushes, width, trajectory_type)\n",
    "    composite_overview = create_ares_overview(dfs, width)\n",
    "\n",
    "    if return_dfs:\n",
    "        return token_view & main_chart & composite_overview, dfs\n",
    "\n",
    "    return token_view & main_chart & composite_overview\n",
    "\n",
    "\n",
    "chart, dfs = plot_adaptive_ews(\n",
    "    results_hosp / \"6982_rep_size_50_2025-01-25_18-29-17\",\n",
    "    results_icu / \"6982_rep_size_50_2025-01-26_04-59-01\",\n",
    "    TrajectoryType.TOKEN,\n",
    "    3,\n",
    "    return_dfs=True,\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files_hosp = os.listdir(results_hosp)\n",
    "files_icu = os.listdir(results_icu)\n",
    "\n",
    "pid_to_file_hosp = {p.split(\"_\")[0]: p for p in files_hosp}\n",
    "pid_to_file_icu = {p.split(\"_\")[0]: p for p in files_icu}\n",
    "\n",
    "ranges_token_view = {\n",
    "    \"7329\": [(28.5, 33.5), (508.5, 513.5), (1000.5, 1005.5)],\n",
    "    \"7110\": [(92.5, 97.5), (508.5, 513.5), (1393.5, 1398.5)],\n",
    "    \"719\": [(110.5, 115.5), (378.5, 383.5), (474.5, 479.5)],\n",
    "    \"415\": [(245.5, 250.5), (375.5, 380.5), (940.5, 945.5)],\n",
    "    \"6387830\": [(245.5, 250.5), (375.5, 380.5), (941.5, 946.5)],\n",
    "}\n",
    "for pid, p_icu in pid_to_file_icu.items():\n",
    "    p_hosp = p_icu if pid not in pid_to_file_hosp else pid_to_file_hosp[pid]\n",
    "    p_full_icu = results_icu / p_icu\n",
    "    p_full_hosp = results_hosp / p_hosp if pid in pid_to_file_hosp else results_icu / p_hosp\n",
    "    ranges_token = ranges_token_view[pid] if pid in ranges_token_view else None\n",
    "    results_path = results_trajectories / pid\n",
    "\n",
    "    if results_path.exists():\n",
    "        continue\n",
    "    print(f\"Generating trajectories for {pid}, {p_icu}\")\n",
    "    results_path.mkdir(parents=True, exist_ok=True)\n",
    "    token_view_window_num = len(ranges_token) if ranges_token else 3\n",
    "    chart_token, dfs_token = plot_adaptive_ews(\n",
    "        p_full_hosp,\n",
    "        p_full_icu,\n",
    "        trajectory_type=TrajectoryType.TOKEN,\n",
    "        token_view_window_num=token_view_window_num,\n",
    "        token_view_ranges=ranges_token,\n",
    "        return_dfs=True,\n",
    "    )\n",
    "    chart_time, dfs_time = plot_adaptive_ews(\n",
    "        p_full_hosp,\n",
    "        p_full_icu,\n",
    "        trajectory_type=TrajectoryType.TIME,\n",
    "        token_view_window_num=3,\n",
    "        return_dfs=True,\n",
    "    )\n",
    "    chart_grouped, dfs_token_grouped = plot_adaptive_ews(\n",
    "        p_full_hosp,\n",
    "        p_full_icu,\n",
    "        trajectory_type=TrajectoryType.TOKEN_GROUPED,\n",
    "        token_view_window_num=3,\n",
    "        return_dfs=True,\n",
    "    )\n",
    "    chart_token_1 = plot_adaptive_ews(\n",
    "        p_full_hosp,\n",
    "        p_full_icu,\n",
    "        trajectory_type=TrajectoryType.TOKEN,\n",
    "        token_view_window_num=1,\n",
    "        token_view_ranges=[ranges_token[-1]] if ranges_token else None,\n",
    "        return_dfs=False,\n",
    "    )\n",
    "\n",
    "    chart_token.save(results_path / f\"ares_{pid}_token.html\")\n",
    "    chart_time.save(results_path / f\"ares_{pid}_time.html\")\n",
    "    chart_grouped.save(results_path / f\"ares_{pid}_token_grouped.html\")\n",
    "    chart_token_1.save(results_path / f\"ares_{pid}_token_1.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
