{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Figures for the ARES paper\n",
    "\n",
    "Requires additional dependencies, run `pip install -e .[jupyter]`\n",
    "\n",
    "In order to run the notebook, you need to have the MIMIC-IV 2.2 dataset with the ED extension.\n",
    "And run the MEDS extraction pipeline, refer to README for the instructions how to do it. Afterwards, adjust the below path to match your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.constants import PROJECT_ROOT\n",
    "\n",
    "mimic_dir = PROJECT_ROOT / \"data/mimic-2.2\"\n",
    "mimic_meds_dir = PROJECT_ROOT / \"data/mimic-2.2-meds-ed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from enum import StrEnum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from pylatex import NoEscape, Table, Tabular, escape_latex\n",
    "\n",
    "from ethos.constants import SpecialToken as ST\n",
    "from ethos.inference.constants import Reason, Task\n",
    "from ethos.metrics import preprocess_inference_results\n",
    "\n",
    "\n",
    "def make_bold(s):\n",
    "    return NoEscape(r\"\\textbf{\" + escape_latex(s) + \"}\")\n",
    "\n",
    "\n",
    "split_titles = {\n",
    "    \"train\": \"Train/Validation\",\n",
    "    \"test\": \"Test\",\n",
    "    \"total\": \"Total\",\n",
    "}\n",
    "\n",
    "n_bootstraps = 10\n",
    "\n",
    "sns.set_theme(context=\"paper\", style=\"white\")\n",
    "\n",
    "# Colors\n",
    "black_color = \"#404040ff\"\n",
    "gray_color = \"#b2b2b2ff\"\n",
    "orange_color = \"#ff8533ff\"\n",
    "font_size = 18\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.rcParams[\"axes.labelcolor\"] = black_color\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.titlecolor\"] = black_color\n",
    "plt.rcParams[\"axes.titlesize\"] = font_size\n",
    "plt.rcParams[\"axes.titleweight\"] = \"bold\"\n",
    "plt.rcParams[\"figure.labelsize\"] = font_size\n",
    "plt.rcParams[\"figure.labelweight\"] = \"bold\"\n",
    "plt.rcParams[\"font.family\"] = \"Roboto\"  # has to be installed on the system\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"text.color\"] = black_color\n",
    "plt.rcParams[\"xtick.color\"] = black_color\n",
    "plt.rcParams[\"xtick.labelsize\"] = font_size\n",
    "plt.rcParams[\"ytick.color\"] = black_color\n",
    "plt.rcParams[\"ytick.labelsize\"] = font_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Patient Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires original MIMIC-IV 2.2 dataset that we do not provide\n",
    "# if you have them in the csv.gz format, use `pl.read_csv`, and change format of the files accordingly\n",
    "patients_df = pl.read_parquet(mimic_dir / \"hosp/patients.parquet\")\n",
    "admissions_df = pl.read_parquet(mimic_dir / \"hosp/admissions.parquet\")\n",
    "\n",
    "# This file is automatically created by running the MEDS extraction pipeline, refer to README.\n",
    "# Optionally, it can be downloaded at https://github.com/ipolharvard/mimic4ed-benchmark/blob/main/scripts/data/subject_splits.parquet\n",
    "subject_split_df = pl.read_parquet(mimic_meds_dir / \"metadata/subject_splits.parquet\")\n",
    "\n",
    "\n",
    "def compute_split_counts(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    splits = list(split_titles.keys())\n",
    "    return (\n",
    "        df.group_by(\"split\", \"code\")\n",
    "        .agg(pl.count(\"code\").alias(\"count\"))\n",
    "        .pivot(\"split\", index=\"code\")\n",
    "        .with_columns(total=pl.sum_horizontal(*splits[:2]))\n",
    "        .select(\"code\", *splits)\n",
    "        .sort(splits[-1], descending=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def add_percentages(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return df.with_columns(\n",
    "        pl.col(col).map_elements(lambda s: f\"{s:,}\", return_dtype=pl.String)\n",
    "        + (\n",
    "            (pl.col(col) / pl.sum(col)).map_elements(\n",
    "                lambda s: f\" ({s * 100:.1f})\", return_dtype=pl.String\n",
    "            )\n",
    "        )\n",
    "        for col in split_titles.keys()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient_num = patients_df.join(subject_split_df, on=\"subject_id\").with_columns(\n",
    "    code=pl.lit(\"Patient Number\")\n",
    ")\n",
    "df_patient_num = compute_split_counts(df_patient_num)\n",
    "df_patient_num = df_patient_num.with_columns(\n",
    "    pl.exclude(\"code\").map_elements(lambda s: f\"{s:,}\", return_dtype=pl.String)\n",
    ")\n",
    "df_patient_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = patients_df.join(subject_split_df, on=\"subject_id\").with_columns(\n",
    "    code=pl.lit(\"Mean Age (std)\")\n",
    ")\n",
    "df_age = (\n",
    "    pl.concat(\n",
    "        (\n",
    "            df_age.group_by(\"split\", maintain_order=True).agg(\n",
    "                pl.first(\"code\"),\n",
    "                pl.mean(\"anchor_age\").alias(\"mean\"),\n",
    "                pl.std(\"anchor_age\").alias(\"std\"),\n",
    "            ),\n",
    "            df_age.group_by(split=pl.lit(\"total\")).agg(\n",
    "                pl.first(\"code\"),\n",
    "                pl.mean(\"anchor_age\").alias(\"mean\"),\n",
    "                pl.std(\"anchor_age\").alias(\"std\"),\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    .select(\n",
    "        \"split\",\n",
    "        \"code\",\n",
    "        value=pl.col(\"mean\").map_elements(lambda s: f\"{s:.1f}\", return_dtype=pl.String)\n",
    "        + pl.col(\"std\").map_elements(lambda s: f\" ({s:.1f})\", return_dtype=pl.String),\n",
    "    )\n",
    "    .pivot(\"split\", index=\"code\")\n",
    ")\n",
    "df_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_full = (\n",
    "    patients_df.select(\"subject_id\", \"gender\")\n",
    "    .join(subject_split_df, on=\"subject_id\")\n",
    "    .rename({\"gender\": \"code\"})\n",
    ")\n",
    "df_gender_full = df_gender_full.with_columns(\n",
    "    pl.col(\"code\").replace_strict({\"F\": \"Female\", \"M\": \"Male\"})\n",
    ")\n",
    "df_gender = compute_split_counts(df_gender_full)\n",
    "df_gender = add_percentages(df_gender)\n",
    "df_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.tokenize.mimic import DemographicData\n",
    "\n",
    "df_race_full = (\n",
    "    admissions_df.join(subject_split_df, on=\"subject_id\", how=\"right\")\n",
    "    .rename({\"race\": \"text_value\"})\n",
    "    .select(\"subject_id\", pl.col(\"text_value\").fill_null(\"UNKNOWN\"), \"split\", code=pl.lit(\"RACE\"))\n",
    ")\n",
    "df_race_full = DemographicData.process_race(df_race_full).drop(\"text_value\")\n",
    "df_race_full = df_race_full.with_columns(pl.col(\"code\").str.slice(len(\"RACE//\")).str.to_titlecase())\n",
    "df_race = compute_split_counts(df_race_full)\n",
    "df_race = add_percentages(df_race)\n",
    "df_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marital = (\n",
    "    admissions_df.join(subject_split_df, on=\"subject_id\", how=\"right\")\n",
    "    .rename({\"marital_status\": \"code\"})\n",
    "    .sort(\"subject_id\", \"admittime\")\n",
    "    .group_by(\"subject_id\", maintain_order=True)\n",
    "    .agg(pl.first(\"code\", \"split\"))\n",
    "    .with_columns(pl.col(\"code\").fill_null(\"UNKNOWN\"))\n",
    ")\n",
    "df_marital = compute_split_counts(df_marital)\n",
    "df_marital = df_marital.with_columns(pl.col(\"code\").str.to_titlecase())\n",
    "df_marital = add_percentages(df_marital)\n",
    "df_marital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_combined_df = pl.concat(\n",
    "    [\n",
    "        df_patient_num.with_columns(group=pl.lit(\"Patient Number\"), code=None).cast(pl.String),\n",
    "        df_age.with_columns(group=pl.lit(\"Mean Age (std.)\"), code=None),\n",
    "        df_gender.with_columns(group=pl.lit(\"Gender (%)\")),\n",
    "        df_race.with_columns(group=pl.lit(\"Race (%)\")),\n",
    "        df_marital.with_columns(group=pl.lit(\"Marital Status (%)\")),\n",
    "    ]\n",
    ").select(\"group\", \"code\", *split_titles.keys())\n",
    "dem_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table()\n",
    "\n",
    "tabular = Tabular(\"l\" + \"r\" * len(split_titles))\n",
    "tabular.append(NoEscape(r\"\\toprule\"))\n",
    "tabular.add_row(\"\", *[make_bold(t) for t in split_titles.values()])\n",
    "tabular.append(NoEscape(r\"\\toprule\"))\n",
    "\n",
    "last_group = \"\"\n",
    "for row in dem_combined_df.rows():\n",
    "    group, subgroup, values = row[0], row[1], row[2:]\n",
    "\n",
    "    if subgroup is None:\n",
    "        first_cell = make_bold(group.title())\n",
    "    else:\n",
    "        if last_group != group:\n",
    "            tabular.append(NoEscape(r\"\\midrule\"))\n",
    "            tabular.add_row(make_bold(group.title()), *[\"\"] * len(values))\n",
    "            last_group = group\n",
    "        first_cell = NoEscape(r\"\\hspace{1em} \" + subgroup)\n",
    "\n",
    "    tabular.add_row(first_cell, *values)\n",
    "\n",
    "tabular.append(NoEscape(r\"\\bottomrule\"))\n",
    "table.append(NoEscape(r\"\\centering\"))\n",
    "\n",
    "table.add_caption(\n",
    "    NoEscape(\n",
    "        r\"\\textbf{Demographic characteristics of the dataset analyzed in this study.}\"\n",
    "        \" The table summarizes key demographic attributes of the dataset, stratified into \"\n",
    "        \"Train/Validation, Test, and Total splits. Patient numbers, mean age (with standard deviation),\"\n",
    "        \" and distribution across gender, race, and marital status are shown, with percentages \"\n",
    "        \"provided in parentheses. The data highlights the representation of each subgroup within \"\n",
    "        \"the splits, providing context for the population characteristics in the dataset.\"\n",
    "    )\n",
    ")\n",
    "table.append(NoEscape(r\"\\label{tab:population-demographic}\"))\n",
    "table.append(tabular)\n",
    "\n",
    "print(table.dumps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Preparation of ETHOS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperTask(StrEnum):\n",
    "    # tasks to showcase ARES\n",
    "    HOSPITAL_MORTALITY = Task.HOSPITAL_MORTALITY\n",
    "    ICU_ADMISSION = Task.ICU_ADMISSION\n",
    "    PROLONGED_STAY = \"prolonged_stay\"\n",
    "    COMPOSITE = \"composite\"\n",
    "    # tasks from the ED benchmark paper\n",
    "    ED_HOSPITALIZATION = Task.ED_HOSPITALIZATION\n",
    "    ED_CRITICAL_OUTCOME = Task.ED_CRITICAL_OUTCOME\n",
    "    ED_REPRESENTATION = Task.ED_REPRESENTATION\n",
    "\n",
    "\n",
    "task_titles = {\n",
    "    PaperTask.HOSPITAL_MORTALITY: \"Hospital Mortality\",\n",
    "    PaperTask.ICU_ADMISSION: \"ICU Admission\",\n",
    "    PaperTask.PROLONGED_STAY: \"Prolonged Stay\",\n",
    "    PaperTask.COMPOSITE: \"Composite (HM+IA+PS)\",\n",
    "    PaperTask.ED_HOSPITALIZATION: \"Hospitalization At Triage\",\n",
    "    PaperTask.ED_CRITICAL_OUTCOME: \"Critical Outcome\\nWithin 12h At Triage\",\n",
    "    PaperTask.ED_REPRESENTATION: \"ED Re-presentation\\nWithin 72h\",\n",
    "}\n",
    "\n",
    "results_dir = PROJECT_ROOT / \"results\"\n",
    "results_fn = \"mimic_old_ed_layer_6_do_0.3_recent_b0y9njtw\"\n",
    "\n",
    "all_ethos_result_dfs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Hospital Mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ethos_result_dfs[PaperTask.HOSPITAL_MORTALITY] = preprocess_inference_results(\n",
    "    results_dir / Task.HOSPITAL_MORTALITY / results_fn,\n",
    "    actual_expr=pl.col(\"actual\").is_in([ST.DEATH]),\n",
    "    expected_expr=pl.col(\"expected\").is_in([ST.DEATH]),\n",
    "    filter_ambiguous=(\n",
    "        ~pl.col(\"actual\").is_in([ST.TIMELINE_END]) & pl.col(\"stop_reason\").is_in([Reason.GOT_TOKEN])\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### ICU Admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ethos_result_dfs[PaperTask.ICU_ADMISSION] = preprocess_inference_results(\n",
    "    results_dir / Task.ICU_ADMISSION / results_fn,\n",
    "    actual_expr=pl.col(\"actual\").is_in([ST.ICU_ADMISSION]),\n",
    "    expected_expr=pl.col(\"expected\").is_in([ST.ICU_ADMISSION]),\n",
    "    filter_ambiguous=(\n",
    "        ~pl.col(\"actual\").is_in([ST.TIMELINE_END]) & pl.col(\"stop_reason\").is_in([Reason.GOT_TOKEN])\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Prolonged Stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def get_los_quantile(q: float) -> timedelta:\n",
    "    return (\n",
    "        admissions_df.lazy()\n",
    "        .with_columns(\n",
    "            pl.col(\"admittime\", \"dischtime\", \"deathtime\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "        .select((pl.min_horizontal(\"dischtime\", \"deathtime\") - pl.col(\"admittime\")).quantile(q))\n",
    "        .collect()\n",
    "    ).item()\n",
    "\n",
    "\n",
    "# we define the prolonged stay as everything longer than 90th percentile of all lengths of stay\n",
    "print(get_los_quantile(0.9))\n",
    "# we decided to round up the cuttoff to 10 days\n",
    "prolonged_stay_cutoff = timedelta(days=10)\n",
    "\n",
    "all_ethos_result_dfs[PaperTask.PROLONGED_STAY] = preprocess_inference_results(\n",
    "    results_dir / PaperTask.HOSPITAL_MORTALITY / results_fn,\n",
    "    actual_expr=pl.col(\"token_time\") >= prolonged_stay_cutoff,\n",
    "    expected_expr=pl.col(\"true_token_time\") >= prolonged_stay_cutoff,\n",
    "    filter_ambiguous=(\n",
    "        ~pl.col(\"actual\").is_in([ST.TIMELINE_END]) & pl.col(\"stop_reason\").is_in([Reason.GOT_TOKEN])\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Composite: Mortality/ICU Admission/Prolonged Stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ethos_result_dfs[PaperTask.COMPOSITE] = preprocess_inference_results(\n",
    "    results_dir / PaperTask.ICU_ADMISSION / results_fn,\n",
    "    actual_expr=pl.col(\"actual\").is_in([ST.ICU_ADMISSION, ST.DEATH])\n",
    "    | (pl.col(\"token_time\") >= prolonged_stay_cutoff),\n",
    "    expected_expr=pl.col(\"expected\").is_in([ST.ICU_ADMISSION, ST.DEATH])\n",
    "    | (pl.col(\"true_token_time\") >= prolonged_stay_cutoff),\n",
    "    filter_ambiguous=(\n",
    "        ~pl.col(\"actual\").is_in([ST.TIMELINE_END]) & pl.col(\"stop_reason\").is_in([Reason.GOT_TOKEN])\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### ED Hospitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ethos_result_dfs[PaperTask.ED_HOSPITALIZATION] = preprocess_inference_results(\n",
    "    results_dir / PaperTask.ED_HOSPITALIZATION / results_fn,\n",
    "    actual_expr=pl.col(\"actual\").is_in([ST.ADMISSION]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### ED Critical Outcome Within 12h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ethos_result_dfs[PaperTask.ED_CRITICAL_OUTCOME] = preprocess_inference_results(\n",
    "    results_dir / PaperTask.ED_CRITICAL_OUTCOME / results_fn,\n",
    "    actual_expr=pl.col(\"actual\").is_in([ST.ICU_ADMISSION, ST.DEATH]),\n",
    "    expected_expr=pl.col(\"expected\") & (pl.col(\"true_token_time\") <= pl.duration(hours=12)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### ED Reattendance Within 72h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ethos_result_dfs[PaperTask.ED_REPRESENTATION] = preprocess_inference_results(\n",
    "    results_dir / PaperTask.ED_REPRESENTATION / results_fn,\n",
    "    actual_expr=pl.col(\"actual\").is_in([ST.ED_ADMISSION]),\n",
    "    expected_expr=pl.col(\"expected\") & (pl.col(\"true_token_time\") <= pl.duration(hours=72)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Preparation of MEDS-TAB Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meds_tab_results(task: PaperTask) -> pl.DataFrame:\n",
    "    cols_mapping = {\n",
    "        \"subject_id\": \"patient_id\",\n",
    "        \"boolean_value\": \"expected\",\n",
    "        \"predicted_boolean_probability\": \"actual\",\n",
    "    }\n",
    "    return pl.read_parquet(\n",
    "        (PROJECT_ROOT / \"results/baseline_meds_tab\" / task).with_suffix(\".parquet\")\n",
    "    ).rename(cols_mapping)[list(cols_mapping.values())]\n",
    "\n",
    "\n",
    "all_meds_tab_result_dfs = {task: get_meds_tab_results(task) for task in PaperTask}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Preparation of ED-Benchmark Processed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score_and_ci(s: str) -> dict:\n",
    "    res, ci = s.split(\" \")\n",
    "    ci = ci[1:-1].split(\"-\")\n",
    "    return {\n",
    "        \"value\": float(res),\n",
    "        \"ci_lower\": float(ci[0]),\n",
    "        \"ci_upper\": float(ci[1]),\n",
    "    }\n",
    "\n",
    "\n",
    "all_ed_bench_results = {\n",
    "    task: [\n",
    "        {\n",
    "            label.lower(): extract_score_and_ci(value) if \"(\" in value else value\n",
    "            for label, value in res.items()\n",
    "            if label.lower() not in [\"threshold\", \"runtime\"]\n",
    "        }\n",
    "        for res in pl.read_csv(fp).to_dicts()\n",
    "    ]\n",
    "    for task in PaperTask\n",
    "    if (fp := (PROJECT_ROOT / \"results/baseline_ed_bench\" / task).with_suffix(\".csv\")).exists()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Functions for all computing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score, roc_curve\n",
    "\n",
    "from ethos.metrics import compute_fitted_metrics\n",
    "\n",
    "metric_names = {\n",
    "    \"auc\": \"AUROC\",\n",
    "    \"auprc\": \"AUPRC\",\n",
    "    \"sensitivity\": \"Sensitivity\",\n",
    "    \"specificity\": \"Specificity\",\n",
    "}\n",
    "\n",
    "\n",
    "def compute_standard_metrics(y_true, y_pred):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    # find the point closest to (0, 1)\n",
    "    best_idx = np.argmin(np.sqrt((fpr - 0) ** 2 + (tpr - 1) ** 2))\n",
    "    return {\n",
    "        \"auc\": roc_auc_score(y_true, y_pred),\n",
    "        \"auprc\": average_precision_score(y_true, y_pred),\n",
    "        \"sensitivity\": tpr[best_idx],\n",
    "        \"specificity\": 1 - fpr[best_idx],\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(df, n_bootstraps=n_bootstraps, use_fit=True):\n",
    "    \"\"\"Use `use_fir` when there is a low variety of y_probs to get the estimated curve.\"\"\"\n",
    "    metric_func = compute_fitted_metrics if use_fit else compute_standard_metrics\n",
    "    results = {\n",
    "        metric: value.item()\n",
    "        for metric, value in metric_func(*df[\"expected\", \"actual\"]).items()\n",
    "        if metric in metric_names\n",
    "    }\n",
    "    results_subsampled = pl.DataFrame(\n",
    "        {\n",
    "            metric: value\n",
    "            for metric, value in metric_func(\n",
    "                *df.sample(fraction=1, with_replacement=True, seed=seed)[\"expected\", \"actual\"]\n",
    "            ).items()\n",
    "            if metric in metric_names\n",
    "        }\n",
    "        for seed in range(n_bootstraps)\n",
    "    )\n",
    "    return {\n",
    "        metric: {\n",
    "            \"value\": results[metric],\n",
    "            \"ci_lower\": results_subsampled[metric].quantile(0.025),\n",
    "            \"ci_upper\": results_subsampled[metric].quantile(0.975),\n",
    "            \"bootstrap_values\": results_subsampled[metric].to_list(),\n",
    "        }\n",
    "        for metric in metric_names.keys()\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_cohen_d(col1: pl.Expr, col2: pl.Expr) -> pl.Expr:\n",
    "    return (col1.mean() - col2.mean()) / (\n",
    "        ((col1.var(ddof=1) * (col1.count() - 1)) + (col2.var(ddof=1) * (col2.count() - 1)))\n",
    "        / ((col1.count() + col2.count()) - 2)\n",
    "    ).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Evaluation of Tasks showcasing ARES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ares_tasks = [\n",
    "    PaperTask.HOSPITAL_MORTALITY,\n",
    "    PaperTask.ICU_ADMISSION,\n",
    "    PaperTask.PROLONGED_STAY,\n",
    "    PaperTask.COMPOSITE,\n",
    "]\n",
    "\n",
    "\n",
    "def join(df: pl.DataFrame, other: pl.DataFrame) -> pl.DataFrame:\n",
    "    return df.join(\n",
    "        other,\n",
    "        left_on=\"patient_id\",\n",
    "        right_on=\"subject_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_metrics_with_subgroup_break_down(result_dfs, **kwargs) -> pl.DataFrame:\n",
    "    results_combined = []\n",
    "\n",
    "    for task in ares_tasks:\n",
    "        df = result_dfs[task]\n",
    "        *auc_and_ci, _ = compute_metrics(df, **kwargs)[\"auc\"].values()\n",
    "        results_combined.append((task, \"overall\", None, *auc_and_ci))\n",
    "\n",
    "        for group_name, df_group in (\n",
    "            (\"gender\", df_gender_full),\n",
    "            (\"race\", df_race_full),\n",
    "        ):\n",
    "            df_with_group = join(df, df_group)\n",
    "            for subgroup in df_with_group[\"code\"].unique().sort():\n",
    "                df_subgroup = df_with_group.filter(code=subgroup)\n",
    "\n",
    "                *auc_and_ci, auc_bootstraps = compute_metrics(df_subgroup, **kwargs)[\"auc\"].values()\n",
    "                results_combined.append((task, group_name, subgroup, *auc_and_ci))\n",
    "\n",
    "    return pl.DataFrame(\n",
    "        results_combined,\n",
    "        schema=[\"task\", \"group\", \"subgroup\", \"auc\", \"ci_lower\", \"ci_upper\"],\n",
    "        orient=\"row\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Compute metrics for ARES tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs about 15 minutes for 100 bootstraps (45 seconds for 5 bootstraps)\n",
    "ethos_ares_results = compute_metrics_with_subgroup_break_down(all_ethos_result_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_tab_ares_results = compute_metrics_with_subgroup_break_down(\n",
    "    all_meds_tab_result_dfs, use_fit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Table for ETHOS vs MEDS-Tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table()\n",
    "table.append(NoEscape(r\"\\centering\"))\n",
    "\n",
    "tabular = Tabular(\"l\" + \"c\" * len(ares_tasks))\n",
    "tabular.append(NoEscape(r\"\\toprule\"))\n",
    "tabular.add_row(\"\", *[make_bold(task_titles[t]) for t in ares_tasks])\n",
    "tabular.add_row(\n",
    "    NoEscape(r\"\\textit{Prevalence} (\\%)\"),\n",
    "    *[f\"{all_ethos_result_dfs[task]['expected'].mean() * 100:.2f}\" for task in ares_tasks],\n",
    ")\n",
    "\n",
    "for method, ares_results in [(\"ETHOS\", ethos_ares_results), (\"MEDS-Tab\", meds_tab_ares_results)]:\n",
    "\n",
    "    tabular.append(NoEscape(r\"\\toprule\"))\n",
    "    tabular.add_row(\n",
    "        (NoEscape(r\"\\multicolumn{5}{c}{\\textbf{\\Large{\" + method + \"}}}\"),), strict=False\n",
    "    )\n",
    "    tabular.append(NoEscape(r\"\\toprule\"))\n",
    "\n",
    "    rows = (\n",
    "        ares_results.select(\n",
    "            \"task\", \"group\", \"subgroup\", values=pl.concat_list(\"auc\", \"ci_lower\", \"ci_upper\")\n",
    "        )\n",
    "        .pivot(\"task\", index=[\"group\", \"subgroup\"], values=\"values\")\n",
    "        .rows()\n",
    "    )\n",
    "    last_group = \"\"\n",
    "    for group, subgroup, *task_results in rows:\n",
    "        formatted_results = [f\"{v:.3f} [{low:.3f}, {high:.3f}]\" for v, low, high in task_results]\n",
    "\n",
    "        if subgroup is None:\n",
    "            first_cell = make_bold(group.title())\n",
    "        else:\n",
    "            if last_group != group:\n",
    "                tabular.append(NoEscape(r\"\\midrule\"))\n",
    "                tabular.add_row(make_bold(group.title()), *[\"\"] * len(formatted_results))\n",
    "                last_group = group\n",
    "            first_cell = NoEscape(r\"\\hspace{1em} \" + subgroup)\n",
    "\n",
    "        tabular.add_row(first_cell, *formatted_results)\n",
    "\n",
    "tabular.append(NoEscape(r\"\\bottomrule\"))\n",
    "\n",
    "table.add_caption(\n",
    "    NoEscape(\n",
    "        r\"\\textbf{ETHOS performance on ARES tasks with a breakdown for demographic subgroups.}\"\n",
    "        r\" This table presents the predictive performance (AUROC with 95\\% confidence intervals) \"\n",
    "        \"of ETHOS (top) and MEDS-Tab (bottom) for four critical clinical outcomes used in ARES: \"\n",
    "        \"Hospital Mortality, ICU Admission, Prolonged Hospital Stay (>10 days), and a Composite \"\n",
    "        \"Risk Score (HM+IA+PS). The prevalence rates of each outcome are provided for reference. \"\n",
    "        \"Performance metrics are further stratified by gender and race to assess potential disparities \"\n",
    "        \"in model performance across demographic subgroups.\"\n",
    "    )\n",
    ")\n",
    "table.append(NoEscape(r\"\\label{tab:ares-results}\"))\n",
    "table.append(tabular)\n",
    "\n",
    "print(table.dumps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Forest Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, you might get some errors if the Robot font is not installed on your system\n",
    "\n",
    "df = (\n",
    "    ethos_ares_results.select(\n",
    "        \"task\", \"group\", \"subgroup\", ETHOS=pl.concat_list(\"auc\", \"ci_lower\", \"ci_upper\")\n",
    "    )\n",
    "    .join(\n",
    "        meds_tab_ares_results.select(\n",
    "            \"task\",\n",
    "            \"group\",\n",
    "            \"subgroup\",\n",
    "            pl.concat_list(\"auc\", \"ci_lower\", \"ci_upper\").alias(\"MEDS-Tab\"),\n",
    "        ),\n",
    "        on=[\"task\", \"group\", \"subgroup\"],\n",
    "        join_nulls=True,\n",
    "    )\n",
    "    .with_columns(subgroup=pl.coalesce(\"subgroup\", \"group\").str.to_titlecase())\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8), sharex=False)\n",
    "axes = axes.ravel()\n",
    "lw = 3\n",
    "\n",
    "for i, ((task,), task_df) in enumerate(df.group_by(\"task\", maintain_order=True)):\n",
    "    ax = axes[i]\n",
    "\n",
    "    subgroups = list(reversed(task_df[\"subgroup\"]))\n",
    "    y_positions = range(len(subgroups))\n",
    "    ax.set_yticks(list(y_positions))\n",
    "    ax.set_yticklabels(subgroups)\n",
    "\n",
    "    for model, marker, color in [(\"MEDS-Tab\", \"s\", gray_color), (\"ETHOS\", \"D\", orange_color)]:\n",
    "        auc_vals, ci_lower, ci_upper = [], [], []\n",
    "\n",
    "        for y, (m, lo, hi) in zip(y_positions, reversed(task_df[model])):\n",
    "            ax.plot([lo, hi], [y, y], color=color, lw=lw, alpha=0.7)\n",
    "            ax.plot([lo, lo], [y - 0.3, y + 0.3], color=color, lw=lw, alpha=0.7)\n",
    "            ax.plot([hi, hi], [y - 0.3, y + 0.3], color=color, lw=lw, alpha=0.7)\n",
    "            ax.plot(m, y, marker=marker, color=color, markersize=7, label=model if y == 0 else None)\n",
    "\n",
    "    ax.set_title(task_titles[task])\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "fig.supxlabel(\"AUC score (95% CI)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Comparison of ETHOS and MEDS-Tab in subgroups of patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bartlett\n",
    "\n",
    "\n",
    "def get_auc_from_results(results: pl.DataFrame, model: str) -> pl.DataFrame:\n",
    "    return (\n",
    "        results.pivot(\"task\", index=[\"group\", \"subgroup\"], values=\"auc\")\n",
    "        .filter(pl.col(\"group\") != \"overall\")\n",
    "        .group_by(\"group\")\n",
    "        .agg(pl.exclude(\"subgroup\"))\n",
    "        .with_columns(model=pl.lit(model))\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    pl.concat(\n",
    "        [\n",
    "            get_auc_from_results(ethos_ares_results, \"ETHOS\"),\n",
    "            get_auc_from_results(meds_tab_ares_results, \"MEDS-Tab\"),\n",
    "        ]\n",
    "    )\n",
    "    .unpivot(index=[\"group\", \"model\"], variable_name=\"task\")\n",
    "    .pivot(\"model\", index=[\"task\", \"group\"])\n",
    "    .with_columns(\n",
    "        pvalue=pl.struct(\"ETHOS\", \"MEDS-Tab\").map_elements(\n",
    "            lambda d: bartlett(d[\"ETHOS\"], d[\"MEDS-Tab\"])[1], return_dtype=pl.Float64\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## Evaluation of ED Benchmark Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_ed_bench_results(task: PaperTask) -> dict:\n",
    "    results = {\n",
    "        res[\"model\"]: {metric: res[metric] for metric in metric_names.keys()}\n",
    "        for res in all_ed_bench_results[task]\n",
    "    }\n",
    "    results[\"MEDS-Tab\"] = compute_metrics(all_meds_tab_result_dfs[task], use_fit=False)\n",
    "    results[\"ETHOS (ours)\"] = compute_metrics(all_ethos_result_dfs[task], use_fit=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "all_ed_bench_task_results = {}\n",
    "\n",
    "\n",
    "def format_results(results: dict) -> str:\n",
    "    return f\"{results['value']:.3f} [{results['ci_lower']:.3f}, {results['ci_upper']:.3f}]\"\n",
    "\n",
    "\n",
    "def construct_latex_table(task: PaperTask, caption: str) -> str:\n",
    "    all_ed_bench_task_results[task] = gather_ed_bench_results(task)\n",
    "    return (\n",
    "        pl.DataFrame(\n",
    "            [\n",
    "                (model, *[format_results(metric_results[metric]) for metric in metric_names.keys()])\n",
    "                for model, metric_results in all_ed_bench_task_results[task].items()\n",
    "            ],\n",
    "            orient=\"row\",\n",
    "            schema=[\" \", *metric_names.values()],\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .to_latex(\n",
    "            index=False,\n",
    "            column_format=\"lcccc\",\n",
    "            escape=True,\n",
    "            caption=caption,\n",
    "            label=f\"tab:{task.replace(\"_\", \"-\")}\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Figure of ED Hospitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    construct_latex_table(\n",
    "        PaperTask.ED_HOSPITALIZATION,\n",
    "        f\"{make_bold('Prediction of Hospitalization At Triage.')} Performance comparison of various models for predicting hospitalization at triage, \"\n",
    "        r\"evaluated using AUROC, AUPRC, sensitivity, and specificity (95\\% confidence intervals in brackets). The thresholds for\"\n",
    "        \" sensitivity and specificity were determined by finding the operating point on the ROC curve closest to (0,1). ETHOS \"\n",
    "        \"demonstrates superior performance across all metrics, achieving the highest AUROC (0.912), AUPRC (0.887), sensitivity (0.849), \"\n",
    "        \"and specificity (0.820), outperforming all other methods, including traditional scoring systems and machine learning models.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### Figure of Critical Outcome Within 12h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    construct_latex_table(\n",
    "        PaperTask.ED_CRITICAL_OUTCOME,\n",
    "        f\"{make_bold('Prediction of Critical Outcome Within 12h At Triage.')} Performance comparison of various models for predicting critical outcomes \"\n",
    "        r\"within 12 hours of triage, evaluated using AUROC, AUPRC, sensitivity, and specificity (95\\% confidence intervals in brackets). The thresholds \"\n",
    "        \"for sensitivity and specificity were determined by finding the operating point on the ROC curve closest to (0,1). ETHOS achieves the \"\n",
    "        \"highest performance across most of the metrics, with an AUROC of 0.937, AUPRC of 0.649, sensitivity of 0.858, and specificity of 0.863, substantially \"\n",
    "        \"outperforming all other methods, including traditional scoring systems and machine learning models.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Figure of ED Re-presentation Within 72h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    construct_latex_table(\n",
    "        PaperTask.ED_REPRESENTATION,\n",
    "        f\"{make_bold('Prediction of Emergency Department Re-presentation Within 72h.')} Performance comparison of various models for predicting emergency department\"\n",
    "        r\" re-presentation within 72 hours, evaluated using AUROC, AUPRC, sensitivity, and specificity (95\\% confidence intervals in brackets). The thresholds for \"\n",
    "        \"sensitivity and specificity were determined by finding the operating point on the ROC curve closest to (0,1). ETHOS demonstrates superior performance, \"\n",
    "        \"achieving the highest AUROC (0.740), AUPRC (0.199), sensitivity (0.659), and specificity (0.696), outperforming all other methods and showcasing its effectiveness for this challenging task.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "\n",
    "### Forest Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = 2, 2\n",
    "fig, axes = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(10, 9))\n",
    "lw = 3\n",
    "\n",
    "for i, (task, results) in enumerate(all_ed_bench_task_results.items()):\n",
    "    ax = axes.ravel()[i]\n",
    "    y_positions = list(reversed(range(len(results))))\n",
    "\n",
    "    results = {\n",
    "        model: res[\"auc\"]\n",
    "        for model, res in sorted(results.items(), key=lambda x: x[1][\"auc\"][\"value\"])\n",
    "    }\n",
    "\n",
    "    for y, auc_score in zip(y_positions, results.values()):\n",
    "        m, lo, hi = auc_score[\"value\"], auc_score[\"ci_lower\"], auc_score[\"ci_upper\"]\n",
    "\n",
    "        ax.plot([lo, hi], [y, y], color=orange_color, lw=lw)\n",
    "        ax.plot([lo, lo], [y - 0.3, y + 0.3], color=orange_color, lw=lw)\n",
    "        ax.plot([hi, hi], [y - 0.3, y + 0.3], color=orange_color, lw=lw)\n",
    "        ax.plot(m, y, marker=\"D\", color=orange_color, markersize=6)\n",
    "\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(results.keys())\n",
    "    ax.grid(True)\n",
    "    ax.set_title(task_titles[task])\n",
    "\n",
    "while (i := i + 1) < n_rows * n_cols:\n",
    "    ax = axes.flatten()[i].set_visible(False)\n",
    "\n",
    "fig.supxlabel(\"AUC score (95% CI)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Tokenized MIMIC dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part uses tokenized MIMIC dataset, refer to README for instructions how to run it\n",
    "dataset_dir = PROJECT_ROOT / \"data/tokenized_datasets/mimic_ed\"\n",
    "\n",
    "test_counts = pl.read_csv(dataset_dir / \"test/code_counts.csv\")\n",
    "train_counts = pl.read_csv(dataset_dir / \"train/code_counts.csv\")\n",
    "total_counts = pl.concat([test_counts, train_counts]).group_by(\"code\").agg(pl.sum(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Simple PHT Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.datasets import TimelineDataset\n",
    "\n",
    "lengths = []\n",
    "for fold in [\"train\", \"test\"]:\n",
    "    patient_offsets = TimelineDataset(dataset_dir / fold).patient_offsets.numpy()\n",
    "    lengths.append(pl.Series(patient_offsets[1:] - patient_offsets[:-1]))\n",
    "train_pht_lengths, test_pht_lengths = lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_timeline_lengths(series, func):\n",
    "    return func(series)\n",
    "\n",
    "\n",
    "timeline_lengths_exprs = {\n",
    "    (\"Tokens\", None): lambda s: s.sum(),\n",
    "    (\"Timelines\", None): lambda s: s.len(),\n",
    "    (\"Timeline Lengths\", \"Longest\"): lambda s: s.max(),\n",
    "    (\"Timeline Lengths\", \"Q3\"): lambda s: s.quantile(0.75),\n",
    "    (\"Timeline Lengths\", \"Median\"): lambda s: s.median(),\n",
    "    (\"Timeline Lengths\", \"Mean\"): lambda s: s.mean(),\n",
    "    (\"Timeline Lengths\", \"Q1\"): lambda s: s.quantile(0.25),\n",
    "    (\"Timeline Lengths\", \"Shortest\"): lambda s: s.min(),\n",
    "    (\"Timeline Lengths\", \"Unique\"): lambda s: s.n_unique(),\n",
    "}\n",
    "\n",
    "timeline_lengths_dfs = [\n",
    "    train_pht_lengths,\n",
    "    test_pht_lengths,\n",
    "    pl.concat([train_pht_lengths, test_pht_lengths]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_code_counts(df, func):\n",
    "    return df.select(func(pl.col(\"code\")).sum()).item()\n",
    "\n",
    "\n",
    "filtering_exprs = {\n",
    "    (\"Unique Timeline Tokens\", None): lambda s: s.len(),\n",
    "    (\"Timeline Tokens Encoding\", \"Time Intervals\"): lambda s: (\n",
    "        s.str.starts_with(\"=\") | s.str.contains(\"-\")\n",
    "    )\n",
    "    & ~s.str.contains(\"//\"),\n",
    "    (\"Timeline Tokens Encoding\", \"Quantiles\"): lambda s: s.str.starts_with(\"Q\")\n",
    "    & ~s.str.contains(\"//\"),\n",
    "    (\"Timeline Tokens Encoding\", \"Medications\"): lambda s: s.str.starts_with(\"ATC//\"),\n",
    "    (\"Timeline Tokens Encoding\", \"Diagnoses\"): lambda s: s.str.starts_with(\"ICD//CM\"),\n",
    "    (\"Timeline Tokens Encoding\", \"Procedures\"): lambda s: s.str.starts_with(\"ICD//PCS\"),\n",
    "    (\"Timeline Tokens Encoding\", \"Labs\"): lambda s: s.str.starts_with(\"LAB//\"),\n",
    "    (\"Timeline Tokens Encoding\", \"Vitals\"): lambda s: s.str.starts_with(\"VITAL//\"),\n",
    "    (\"Timeline Tokens Encoding\", \"HCPCS\"): lambda s: s.str.starts_with(\"HCPCS//\"),\n",
    "    (\"Timeline Tokens Encoding\", \"Inpatient Stays\"): lambda s: s.str.starts_with(\"HOSPITAL_\")\n",
    "    | s.str.starts_with(\"ICU_\")\n",
    "    | s.str.starts_with(\"DISCHARGE_\")\n",
    "    | s.str.starts_with(\"INSURANCE\")\n",
    "    | s.str.starts_with(\"ADMISSION_\"),\n",
    "    (\"Timeline Tokens Encoding\", \"Emergency Department\"): lambda s: s.str.starts_with(\"ED_\"),\n",
    "    (\"Timeline Tokens Encoding\", \"DRGs\"): lambda s: s.str.starts_with(\"DRG\"),\n",
    "    (\"Timeline Tokens Encoding\", \"BMI\"): lambda s: s.str.starts_with(\"BMI//\"),\n",
    "}\n",
    "\n",
    "code_count_dfs = [train_counts, test_counts, total_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_pht_numbers_df = pl.DataFrame(\n",
    "    [\n",
    "        (*label, *[func(df, expr) for df in dfs])\n",
    "        for func, exprs, dfs in [\n",
    "            (eval_timeline_lengths, timeline_lengths_exprs, timeline_lengths_dfs),\n",
    "            (eval_code_counts, filtering_exprs, code_count_dfs),\n",
    "        ]\n",
    "        for label, expr in exprs.items()\n",
    "    ],\n",
    "    schema={\"group\": str, \"subgroup\": str, \"train\": int, \"test\": int, \"total\": int},\n",
    "    orient=\"row\",\n",
    ")\n",
    "\n",
    "table = Table()\n",
    "tabular = Tabular(\"lrrr\")\n",
    "tabular.append(NoEscape(r\"\\toprule\"))\n",
    "tabular.add_row(\"\", *[make_bold(s) for s in split_titles.values()])\n",
    "tabular.append(NoEscape(r\"\\toprule\"))\n",
    "\n",
    "last_group = \"\"\n",
    "for group, subgroup, *values in general_pht_numbers_df.rows():\n",
    "\n",
    "    if subgroup is None:\n",
    "        first_cell = make_bold(group)\n",
    "        if last_group != \"\":\n",
    "            tabular.append(NoEscape(r\"\\midrule\"))\n",
    "    else:\n",
    "        if last_group != group:\n",
    "            if last_group != \"\":\n",
    "                tabular.append(NoEscape(r\"\\midrule\"))\n",
    "            tabular.add_row((make_bold(group),), strict=False)\n",
    "        first_cell = NoEscape(r\"\\hspace{1em} \" + escape_latex(subgroup))\n",
    "\n",
    "    tabular.add_row(first_cell, *(f\"{v:,}\" for v in values))\n",
    "    last_group = group\n",
    "\n",
    "tabular.append(NoEscape(r\"\\bottomrule\"))\n",
    "table.append(NoEscape(r\"\\centering\"))\n",
    "\n",
    "table.add_caption(\n",
    "    NoEscape(\n",
    "        r\"\\textbf{Summary of Token and Timeline Statistics.} \"\n",
    "        \"This table presents a comprehensive overview of the token and timeline data in the training, test, and combined datasets.\"\n",
    "        \" Key metrics include the total number of tokens and timelines, along with statistics on timeline lengths such as the longest timeline, median, mean, and shortest timeline.\"\n",
    "        \" The number of unique timeline tokens is also reported. The final section breaks down the encoding of timeline tokens into categories, such as time intervals, \"\n",
    "        \"quantiles, medications, diagnoses, procedures, laboratory results, vitals, and other clinical features. \"\n",
    "        \"This summary highlights the diversity and complexity of the tokenized data used in the study.\"\n",
    "    )\n",
    ")\n",
    "table.append(NoEscape(r\"\\label{tab:simple-pht-stats}\"))\n",
    "table.append(tabular)\n",
    "\n",
    "print(table.dumps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### Detailed Token Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_col, unique_col = \"Count\", \"#Unique\"\n",
    "\n",
    "\n",
    "def get_token_contribution(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.group_by(\n",
    "            pl.when(\n",
    "                pl.col(\"code\").str.starts_with(\"ATC\")\n",
    "                & ~pl.col(\"code\").str.starts_with(\"ATC//4//\")\n",
    "                & ~pl.col(\"code\").str.starts_with(\"ATC//SFX//\")\n",
    "            )\n",
    "            .then(pl.lit(\"ATC\"))\n",
    "            .when(pl.col(\"code\").str.slice(0, 3).is_in([\"ICD\", \"ATC\"]))\n",
    "            .then(\n",
    "                pl.col(\"code\").str.slice(0, 3)\n",
    "                + pl.lit(\"_\")\n",
    "                + pl.col(\"code\").str.split(\"//\").list.get(1, null_on_oob=True)\n",
    "            )\n",
    "            .otherwise(pl.col(\"code\").str.split(\"//\").list.get(0))\n",
    "            .alias(\"code\")\n",
    "        )\n",
    "        .agg(pl.sum(\"count\").alias(count_col), pl.count(\"count\").alias(unique_col))\n",
    "        .sort(count_col, descending=True)\n",
    "    )\n",
    "\n",
    "\n",
    "fold_order = {v: i for i, v in enumerate([\"Train\", \"Test\", \"Total\"])}\n",
    "count_results_df = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            get_token_contribution(df).with_columns(split=pl.lit(label))\n",
    "            for label, df in zip(split_titles.values(), [train_counts, test_counts, total_counts])\n",
    "        ]\n",
    "    )\n",
    "    .to_pandas()\n",
    "    .pivot(index=\"code\", columns=\"split\", values=[count_col, unique_col])\n",
    "    .swaplevel(axis=1)\n",
    "    .sort_index(axis=1, key=lambda index: index.map(fold_order), level=0)\n",
    "    .sort_values((\"Total\", count_col), ascending=False)\n",
    ")\n",
    "\n",
    "count_results_df.columns.names = [\"\", \"\"]\n",
    "count_results_df.index.name = \"Code Group\"\n",
    "\n",
    "print(\n",
    "    count_results_df.map(lambda s: f\"{s:,}\").to_latex(\n",
    "        column_format=\"l\" + \"c\" * len(count_results_df.columns),\n",
    "        label=\"tab:token-stats\",\n",
    "        caption=\"\\\\textbf{Token Statistics}. The table provides a detailed breakdown of the total number\"\n",
    "        \" of tokens and unique tokens for each code group in the training, test, and combined datasets.\"\n",
    "        \" Each code group represents a specific type of information, such as laboratory results (LAB),\"\n",
    "        r\" clinical classifications (e.g., ATC, ICD\\_CM), time intervals (e.g., 15m-45m, 12h-18h), and\"\n",
    "        \" other key features like BMI, vitals, or discharge locations. The statistics summarize the diversity\"\n",
    "        r\" (\\#Unique) and frequency (Count) of tokens across datasets, offering insights into the distribution\"\n",
    "        \" and variability of features used in the modeling process.\",\n",
    "        multicolumn_format=\"c\",\n",
    "        escape=True,\n",
    "        longtable=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "## Data sources used to generate the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with (PROJECT_ROOT / \"scripts/meds/mimic/configs/event_configs-ed.yaml\").open(\"r\") as f:\n",
    "    event_configs = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def load_column_names(fp):\n",
    "    return (pl.read_parquet if fp.suffix == \".parquet\" else pl.read_csv)(fp, n_rows=0).columns\n",
    "\n",
    "\n",
    "columns_dont_really_used = [\"drg_severity\", \"priority\", \"drg_mortality\", \"language\", \"emar_seq\"]\n",
    "\n",
    "\n",
    "def filter_columns(columns, event_config):\n",
    "    processed_event_config = [\n",
    "        c[4:-1] if c.startswith(\"col(\") else c\n",
    "        for event_values in event_config.values()\n",
    "        for col_code, col in event_values.items()\n",
    "        if col_code != \"time_format\" and col is not None\n",
    "        for c in (col if isinstance(col, list) else [col])\n",
    "    ]\n",
    "    return [\n",
    "        col\n",
    "        for col in processed_event_config\n",
    "        if col in columns and col not in columns_dont_really_used\n",
    "    ]\n",
    "\n",
    "\n",
    "data_src_df = pl.DataFrame(\n",
    "    [\n",
    "        (*s.split(\"/\"), filter_columns(load_column_names(fps[0]), event_configs[s]))\n",
    "        for s in sorted(event_configs.keys())\n",
    "        if (\n",
    "            fps := [\n",
    "                fp\n",
    "                for sfx in [\".parquet\", \".csv.gz\"]\n",
    "                if (fp := (mimic_dir / s).with_suffix(sfx)).exists()\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    schema=[\"group\", \"supgroup\", \"columns\"],\n",
    "    orient=\"row\",\n",
    ")\n",
    "data_src_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table()\n",
    "tabular = Tabular(\"ll\")\n",
    "tabular.append(NoEscape(r\"\\toprule\"))\n",
    "tabular.add_row(\"Data Source\", \"Used Columns\")\n",
    "tabular.append(NoEscape(r\"\\toprule\"))\n",
    "\n",
    "last_group = \"\"\n",
    "for group, subgroup, columns in data_src_df.rows():\n",
    "\n",
    "    if subgroup is None:\n",
    "        first_cell = make_bold(group)\n",
    "    else:\n",
    "        if last_group != group:\n",
    "            if last_group != \"\":\n",
    "                tabular.append(NoEscape(r\"\\midrule\"))\n",
    "            tabular.add_row((make_bold(group), \"\"))\n",
    "            last_group = group\n",
    "        first_cell = NoEscape(r\"\\hspace{1em} \" + escape_latex(subgroup))\n",
    "        tabular.append(NoEscape(r\"\\vspace{0.2em}\"))\n",
    "\n",
    "    tabular.add_row(\n",
    "        first_cell,\n",
    "        NoEscape(\n",
    "            r\"\\makecell[l]{\"\n",
    "            + r\"\\\\\".join(\n",
    "                [escape_latex(\", \".join(columns[i : i + 3])) for i in range(0, len(columns), 3)]\n",
    "            )\n",
    "            + r\"}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "tabular.append(NoEscape(r\"\\bottomrule\"))\n",
    "table.append(NoEscape(r\"\\centering\"))\n",
    "\n",
    "table.add_caption(\n",
    "    NoEscape(\n",
    "        r\"\\textbf{Overview of the data sources and their corresponding columns used in this work from the MIMIC-IV database and its extension MIMIC-IV-ED.}\"\n",
    "        \" The table groups the data into three main categories: ED (Emergency Department), hosp (Hospital), and ICU (Intensive Care Unit).\"\n",
    "        \" For each category, the associated tables and the specific columns extracted for the study are listed,\"\n",
    "        r\" highlighting key variables relevant to patient care and outcomes, such as identifiers (e.g., stay\\_id, hadm\\_id),\"\n",
    "        \" timestamps (e.g., intime, charttime), and clinical observations (e.g., vitalsign, labresults).\"\n",
    "        \" These selections were guided by the objectives of the study to comprehensively model patient trajectories and outcomes.\"\n",
    "    )\n",
    ")\n",
    "table.append(NoEscape(r\"\\label{tab:data-sources}\"))\n",
    "table.append(tabular)\n",
    "\n",
    "print(table.dumps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## Figure of AUROC for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_attributes(score_name: str) -> tuple[str, str, str]:\n",
    "    match score_name.lower():\n",
    "        case \"auc\":\n",
    "            return \"AUC\", \"fpr_values\", \"tpr_values\"\n",
    "        case \"auprc\":\n",
    "            return \"AUPRC\", \"recall_values\", \"precision_values\"\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown score name: {score_name}\")\n",
    "\n",
    "\n",
    "def compute_ci(df: pl.DataFrame, score_type: str = \"auc\", n_bootstraps: int = n_bootstraps):\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    aucs, tprs = [], []\n",
    "    _, x_attr, y_attr = get_proper_attributes(score_type)\n",
    "    for seed in range(n_bootstraps):\n",
    "        res_fit = compute_fitted_metrics(\n",
    "            *df[\"expected\", \"actual\"].sample(fraction=1, with_replacement=True, seed=seed)\n",
    "        )\n",
    "        aucs.append(res_fit[score_type])\n",
    "        tprs.append(np.interp(mean_fpr, res_fit[x_attr], res_fit[y_attr]))\n",
    "        tprs[-1][0] = 0.0\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_tpr, std_tpr = np.mean(tprs, axis=0), np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    return np.percentile(aucs, [2.5, 97.5]), mean_fpr, tprs_lower, tprs_upper\n",
    "\n",
    "\n",
    "def plot_auc(\n",
    "    df: pl.DataFrame,\n",
    "    score_type: str = \"auc\",\n",
    "    title: str = \"\",\n",
    "    text_upper: bool = True,\n",
    "    n_bootstraps=n_bootstraps,\n",
    "):\n",
    "    res_fit = compute_fitted_metrics(df[\"expected\"], df[\"actual\"])\n",
    "\n",
    "    score_name, x_attr, y_attr = get_proper_attributes(score_type)\n",
    "\n",
    "    if score_type == \"auc\":\n",
    "        (ci_lower, ci_upper), *ci_boundries = compute_ci(df, score_type, n_bootstraps=n_bootstraps)\n",
    "        plt.fill_between(\n",
    "            *ci_boundries,\n",
    "            color=black_color,\n",
    "            alpha=0.5,\n",
    "            label=\"95% Confidence Interval\",\n",
    "        )\n",
    "\n",
    "    plt.scatter(\n",
    "        res_fit[x_attr],\n",
    "        res_fit[y_attr],\n",
    "        marker=\"X\",\n",
    "        color=black_color,\n",
    "        s=100,\n",
    "        label=\"Unique Thresholds\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        res_fit[x_attr],\n",
    "        res_fit[y_attr],\n",
    "        color=orange_color,\n",
    "        lw=5,\n",
    "        label=f\"{score_name} curve\",\n",
    "    )\n",
    "\n",
    "    plt.grid(visible=False)\n",
    "    plt.gca().set(ylim=(-0.01, 1.01), xlim=(-0.01, 1.01), title=title)\n",
    "\n",
    "    text = [f\"{score_name}: {res_fit[score_type]:.3f}\"]\n",
    "    if score_type == \"auc\":\n",
    "        text.append(f\"{score_name} CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "    text.append(f\"N: {len(df):,} ({df['expected'].mean():.1%} pos.)\")\n",
    "\n",
    "    plt.gca().add_artist(\n",
    "        AnchoredText(\n",
    "            \"\\n\".join(text),\n",
    "            loc=f\"{'upper' if text_upper else 'lower'} right\",\n",
    "            pad=0.1,\n",
    "            borderpad=0.1,\n",
    "            frameon=False,\n",
    "            prop=dict(size=font_size, color=black_color),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_ethos_curves(score_type: str = \"auc\", text_upper: tuple[int] = (), **kwargs):\n",
    "    n_rows, n_cols = 2, 4\n",
    "    size = 18\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(size, size * n_rows / n_cols))\n",
    "\n",
    "    for i, (task, df) in enumerate(all_ethos_result_dfs.items()):\n",
    "        ax: plt.Axes = axes[i // n_cols, i % n_cols]\n",
    "        plt.sca(ax)\n",
    "\n",
    "        plot_auc(\n",
    "            df, score_type=score_type, title=task_titles[task], text_upper=i in text_upper, **kwargs\n",
    "        )\n",
    "\n",
    "        if i % n_cols != 0:\n",
    "            ax.set_ylabel(\"\")\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        if i < n_cols:\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_xticks([])\n",
    "    else:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(\n",
    "            handles[::-1],\n",
    "            labels[::-1],\n",
    "            loc=\"upper left\",\n",
    "            frameon=False,\n",
    "            fontsize=font_size,\n",
    "            bbox_to_anchor=(1.05, 0.8),\n",
    "        )\n",
    "        while (i := i + 1) < n_rows * n_cols:\n",
    "            ax = axes.flatten()[i].set_visible(False)\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ethos_curves(score_type=\"auc\", n_bootstraps=n_bootstraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ethos_curves(score_type=\"auprc\", text_upper=(0, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## Figure of Calibration Curves for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "n_rows, n_cols = 2, 4\n",
    "size = 18\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(size, size * n_rows / n_cols))\n",
    "\n",
    "n_bins = 10\n",
    "\n",
    "for i, (task, df) in enumerate(all_ethos_result_dfs.items()):\n",
    "    plt.sca(ax := axes[i // n_cols, i % n_cols])\n",
    "\n",
    "    frac_pos, mean_pred = calibration_curve(*df[\"expected\", \"actual\"], n_bins=n_bins)\n",
    "\n",
    "    bootstrapped_fracs = np.zeros((n_bootstraps, len(mean_pred)))\n",
    "    for seed in range(n_bootstraps):\n",
    "        frac_bs, mean_pred_bs = calibration_curve(\n",
    "            *df[\"expected\", \"actual\"].sample(fraction=1, with_replacement=True, seed=seed),\n",
    "            n_bins=n_bins,\n",
    "        )\n",
    "        bootstrapped_fracs[seed] = (\n",
    "            frac_bs\n",
    "            if len(frac_bs) == len(frac_pos)\n",
    "            else np.interp(mean_pred, mean_pred_bs, frac_bs)\n",
    "        )\n",
    "\n",
    "    ci_lower, ci_upper = np.percentile(bootstrapped_fracs, [2.5, 97.5], axis=0)\n",
    "    plt.fill_between(\n",
    "        mean_pred,\n",
    "        ci_lower,\n",
    "        ci_upper,\n",
    "        color=gray_color,\n",
    "        label=\"95% Confidence Interval\",\n",
    "    )\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=black_color, label=\"Perfect Calibration\")\n",
    "    plt.plot(\n",
    "        mean_pred,\n",
    "        frac_pos,\n",
    "        color=orange_color,\n",
    "        lw=5,\n",
    "        label=\"ETHOS Calibration\",\n",
    "    )\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.title(task_titles[task])\n",
    "    plt.grid(False)\n",
    "\n",
    "    at = AnchoredText(\n",
    "        f\"Brier score: {brier_score_loss(df['expected'], df['actual']):.3f}\",\n",
    "        loc=\"lower right\",\n",
    "        pad=0,\n",
    "        borderpad=0.1,\n",
    "        frameon=False,\n",
    "        prop=dict(size=font_size, color=black_color),\n",
    "    )\n",
    "    ax.add_artist(at)\n",
    "\n",
    "    if i % n_cols != 0:\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    if i < n_cols:\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_xticks([])\n",
    "else:\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles[::-1],\n",
    "        labels[::-1],\n",
    "        loc=\"center right\",\n",
    "        frameon=False,\n",
    "        fontsize=font_size,\n",
    "        bbox_to_anchor=(2.25, 0.8),\n",
    "    )\n",
    "    while (i := i + 1) < n_rows * n_cols:\n",
    "        ax = axes.flatten()[i].set_visible(False)\n",
    "\n",
    "fig.supxlabel(\"Mean predicted probability\")\n",
    "fig.supylabel(\"Fraction of positives\", x=0.07)\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
